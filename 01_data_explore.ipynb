import os
print(os.getcwd())

print(os.listdir("../data"))

import zipfile

with zipfile.ZipFile("../data/archive.zip", "r") as zip_ref:
    zip_ref.extractall("../data/")

import os
print(os.listdir("../data"))

import pandas as pd

# adjust path with ../ because you’re inside notebooks/
train = pd.read_csv("../data/mitbih_train.csv", header=None)
test  = pd.read_csv("../data/mitbih_test.csv", header=None)

print("Train shape:", train.shape)
print("Test shape:", test.shape)

X_train = train.iloc[:,:-1].values
y_train = train.iloc[:,-1].values

X_test = test.iloc[:,:-1].values
y_test = test.iloc[:,-1].values

print("X_train:", X_train.shape, "y_train:", y_train.shape)
print("X_test:", X_test.shape, "y_test:", y_test.shape)

import numpy as np

unique, counts = np.unique(y_train, return_counts=True)
print(dict(zip(unique, counts)))

import matplotlib.pyplot as plt
import numpy as np

plt.figure(figsize=(12,6))
for i in range(5):
    plt.subplot(1,5,i+1)
    # pick the first example of class i
    idx = np.where(y_train==i)[0][0]
    plt.plot(X_train[idx])
    plt.title(f"Class {i}")
plt.tight_layout()
plt.show()

def zscore(a):
    return (a - a.mean(axis=1, keepdims=True)) / (a.std(axis=1, keepdims=True) + 1e-8)

X_train_norm = zscore(X_train)
X_test_norm  = zscore(X_test)

import sklearn
print(sklearn.__version__)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

clf = RandomForestClassifier(
    n_estimators=200, 
    class_weight="balanced", 
    random_state=42
)
clf.fit(X_train_norm, y_train)

y_pred = clf.predict(X_test_norm)
print(classification_report(y_test, y_pred))

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("RandomForest Confusion Matrix")
plt.show()

from imblearn.over_sampling import SMOTE

sm = SMOTE(random_state=42)
X_res, y_res = sm.fit_resample(X_train_norm, y_train)

print("Before:", np.bincount(y_train.astype(int)))
print("After:",  np.bincount(y_res.astype(int)))

clf_sm = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)
clf_sm.fit(X_res, y_res)

y_pred_sm = clf_sm.predict(X_test_norm)
print(classification_report(y_test, y_pred_sm))

import joblib
joblib.dump(clf_sm, "rf_model.joblib")

import numpy as np
from scipy import signal, stats
import pywt

def extract_features(x, fs=360):
    feats = {}
    x = np.asarray(x)

    # --- Statistical ---
    feats["mean"] = np.mean(x)
    feats["std"] = np.std(x)
    feats["rms"] = np.sqrt(np.mean(x**2))
    feats["ptp"] = np.ptp(x)  # peak-to-peak
    feats["skew"] = stats.skew(x)
    feats["kurtosis"] = stats.kurtosis(x)

    # --- Frequency Domain ---
    f, Pxx = signal.welch(x, fs=fs, nperseg=128)
    feats["total_power"] = np.trapz(Pxx, f)
    feats["spec_entropy"] = -np.sum((Pxx/np.sum(Pxx)) * np.log(Pxx/np.sum(Pxx) + 1e-12))

    # Band powers
    bands = [(0.5, 4), (4, 15), (15, 40)]
    for (a, b) in bands:
        idx = (f >= a) & (f <= b)
        feats[f"band_{a}_{b}"] = np.trapz(Pxx[idx], f[idx]) if np.any(idx) else 0

    # --- Wavelet Features ---
    coeffs = pywt.wavedec(x, 'db6', level=3)
    for i, c in enumerate(coeffs):
        feats[f"w_energy_{i}"] = np.sum(c**2)

    return feats
import pandas as pd

# ⚠️ Warning: this may take time for full dataset
N = 10000  # use smaller number first to test
features = [extract_features(x) for x in X_train_norm[:N]]
df_feats = pd.DataFrame(features)
df_feats["label"] = y_train[:N]
df_feats.head()

from sklearn.model_selection import train_test_split

X_train_f, X_val_f, y_train_f, y_val_f = train_test_split(
    Xf, yf,
    stratify=yf,   # ensures class balance in both sets
    test_size=0.2,
    random_state=42
)

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report

# RandomForest
rf_pipe = Pipeline([
    ("scaler", StandardScaler()), 
    ("clf", RandomForestClassifier(n_estimators=200, random_state=42))
])
rf_pipe.fit(X_train_f, y_train_f)
y_pred_rf = rf_pipe.predict(X_val_f)
print("RandomForest:")
print(classification_report(y_val_f, y_pred_rf))

# SVM
svm_pipe = Pipeline([
    ("scaler", StandardScaler()), 
    ("clf", SVC(kernel="rbf", class_weight="balanced"))
])
svm_pipe.fit(X_train_f, y_train_f)
y_pred_svm = svm_pipe.predict(X_val_f)
print("SVM:")
print(classification_report(y_val_f, y_pred_svm))

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_val_f, y_pred_rf)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("RandomForest (Features)")
plt.show()

features_full = [extract_features(x) for x in X_train_norm]
df_feats_full = pd.DataFrame(features_full)
df_feats_full["label"] = y_train

from sklearn.model_selection import train_test_split

Xf = df_feats_full.drop(columns=["label"]).values
yf = df_feats_full["label"].values

X_train_f, X_val_f, y_train_f, y_val_f = train_test_split(
    Xf, yf,
    stratify=yf,     # ensures each split has all classes
    test_size=0.2,
    random_state=42
)

print("Train shape:", X_train_f.shape, "Val shape:", X_val_f.shape)
print("Train classes:", np.bincount(y_train_f.astype(int)))
print("Val classes:", np.bincount(y_val_f.astype(int)))

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

rf_pipe = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1))
])

rf_pipe.fit(X_train_f, y_train_f)
y_pred_rf = rf_pipe.predict(X_val_f)

print("RandomForest Results:")
print(classification_report(y_val_f, y_pred_rf))

from sklearn.svm import SVC

svm_pipe = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", SVC(kernel="rbf", class_weight="balanced"))
])

# If too slow, sample 20k rows from training
X_train_small, _, y_train_small, _ = train_test_split(
    X_train_f, y_train_f, stratify=y_train_f, train_size=20000, random_state=42
)

svm_pipe.fit(X_train_small, y_train_small)
y_pred_svm = svm_pipe.predict(X_val_f)

print("SVM Results:")
print(classification_report(y_val_f, y_pred_svm))

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_val_f, y_pred_rf)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("RandomForest Confusion Matrix (Features)")
plt.show()

importances = rf_pipe.named_steps["clf"].feature_importances_
feature_names = df_feats_full.drop(columns=["label"]).columns

feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)
print(feat_imp.head(10))

plt.figure(figsize=(8,5))
feat_imp.head(15).plot(kind="bar")
plt.title("Top 15 Feature Importances (RandomForest)")
plt.show()



